"""
LLM Integration Module for Snowflake Chat Analysis
Simple integration functions to add to the main snowflake file
"""

import snowflake.snowpark as snowpark
from datetime import datetime, timedelta

# Import all the LLM modules
from snowflake_llm_orchestrator import (
    main_llm_analysis,
    main_llm_test,
    main_llm_validate,
    analyze_llm_single_department
)
from snowflake_llm_config import (
    list_all_departments,
    get_department_prompt_types,
    list_all_output_tables
)


def llm_analysis_full_pipeline(session: snowpark.Session, target_date=None):
    """
    Run the complete LLM analysis pipeline for all departments
    
    Args:
        session: Snowflake session
        target_date: Target date for analysis (defaults to yesterday)
    
    Returns:
        Analysis results
    """
    print("üöÄ STARTING COMPLETE LLM ANALYSIS PIPELINE")
    print("=" * 60)
    
    # Set default target date
    if target_date is None:
        target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    try:
        # Step 1: Validate dependencies
        print("üìã Step 1: Validating dependencies...")
        validation_results = main_llm_validate(session)
        
        if validation_results['overall_status'] != 'SUCCESS':
            print("‚ùå Dependency validation failed - aborting pipeline")
            return {
                'success': False,
                'error': 'Dependency validation failed',
                'validation_results': validation_results
            }
        
        print("‚úÖ Dependencies validated successfully")
        
        # Step 2: Run complete analysis
        print(f"\nü§ñ Step 2: Running LLM analysis for {target_date}...")
        analysis_results = main_llm_analysis(session, target_date)
        
        # Step 3: Generate final summary
        success = 'error' not in analysis_results
        
        pipeline_summary = f"""
üéØ LLM ANALYSIS PIPELINE - COMPLETE
{'=' * 50}
üìÖ Date: {target_date}
‚úÖ Status: {'SUCCESS' if success else 'FAILED'}
üìä Results: {analysis_results.get('statistics', {}).get('total_processed', 0)} conversations processed
üíæ Tables: {len(list_all_output_tables())} output tables created

{'üåü Pipeline completed successfully!' if success else '‚ùå Pipeline failed - check logs for details'}
"""
        
        print(pipeline_summary)
        
        return {
            'success': success,
            'target_date': target_date,
            'pipeline_summary': pipeline_summary,
            'validation_results': validation_results,
            'analysis_results': analysis_results
        }
        
    except Exception as e:
        error_msg = f"Pipeline failed: {str(e)}"
        print(f"‚ùå {error_msg}")
        return {
            'success': False,
            'error': error_msg,
            'target_date': target_date
        }


def llm_analysis_quick_test(session: snowpark.Session, department_name='CC_Resolvers', target_date=None, prompts=['*'], metrics=['*']):
    """
    Run a quick test of LLM analysis on a single department
    
    Args:
        session: Snowflake session
        department_name: Department to test (defaults to CC_Resolvers)
        target_date: Target date for testing
    
    Returns:
        Test results
    """
    print(f"üß™ QUICK LLM TEST - {department_name}")
    print("=" * 50)
    
    if target_date is None:
        target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    try:
        # Validate dependencies first
        validation_results = main_llm_validate(session)
        if validation_results['overall_status'] != 'SUCCESS':
            return {
                'success': False,
                'error': 'Dependency validation failed',
                'validation_results': validation_results
            }
        
        # Run single department analysis
        results = analyze_llm_single_department(session, department_name, target_date, prompts, metrics)
        
        test_summary = f"""
üß™ QUICK TEST RESULTS
{'=' * 30}
Department: {department_name}
Date: {target_date}
Status: {'SUCCESS' if results['success'] else 'FAILED'}
{'Error: ' + results.get('error', '') if not results['success'] else ''}

{'‚úÖ Quick test passed!' if results['success'] else '‚ùå Quick test failed!'}
"""
        
        print(test_summary)
        
        return {
            'success': results['success'],
            'department': department_name,
            'target_date': target_date,
            'test_summary': test_summary,
            'results': results
        }
        
    except Exception as e:
        error_msg = f"Quick test failed: {str(e)}"
        print(f"‚ùå {error_msg}")
        return {
            'success': False,
            'error': error_msg,
            'department': department_name,
            'target_date': target_date
        }


def llm_analysis_status_check(session: snowpark.Session):
    """
    Check the status of LLM analysis system and configuration
    
    Args:
        session: Snowflake session
    
    Returns:
        Status information
    """
    print("üìä LLM ANALYSIS STATUS CHECK")
    print("=" * 40)
    
    try:
        # Get configuration info
        departments = list_all_departments()
        output_tables = list_all_output_tables()
        
        # Count prompts per department
        prompt_counts = {}
        total_prompts = 0
        
        for dept in departments:
            prompts = get_department_prompt_types(dept)
            prompt_counts[dept] = len(prompts)
            total_prompts += len(prompts)
        
        # Check table existence
        existing_tables = 0
        for table in output_tables:
            try:
                session.sql(f"SELECT COUNT(*) FROM LLM_EVAL.PUBLIC.{table}").collect()
                existing_tables += 1
            except:
                pass
        
        status_info = {
            'departments_configured': len(departments),
            'total_prompts': total_prompts,
            'output_tables_defined': len(output_tables),
            'output_tables_existing': existing_tables,
            'prompt_counts_by_department': prompt_counts
        }
        
        status_summary = f"""
üìä LLM ANALYSIS STATUS
{'=' * 30}
üè¢ Departments: {len(departments)} configured
üéØ Prompts: {total_prompts} total prompts
üíæ Tables: {existing_tables}/{len(output_tables)} tables exist
üìã Configuration: {'‚úÖ Valid' if len(departments) > 0 else '‚ùå Invalid'}

Department breakdown:
{chr(10).join([f'  ‚Ä¢ {dept}: {count} prompts' for dept, count in prompt_counts.items()])}
"""
        
        print(status_summary)
        
        return {
            'status': 'healthy' if len(departments) > 0 and total_prompts > 0 else 'unhealthy',
            'status_summary': status_summary,
            'details': status_info
        }
        
    except Exception as e:
        error_msg = f"Status check failed: {str(e)}"
        print(f"‚ùå {error_msg}")
        return {
            'status': 'error',
            'error': error_msg
        }


def llm_analysis_cleanup_tables(session: snowpark.Session, confirm=False):
    """
    Clean up all LLM analysis tables (use with caution!)
    
    Args:
        session: Snowflake session
        confirm: Must be True to actually perform cleanup
    
    Returns:
        Cleanup results
    """
    if not confirm:
        print("‚ö†Ô∏è  DRY RUN: Cleanup not confirmed. Set confirm=True to actually delete tables.")
        tables = list_all_output_tables()
        print(f"Would delete {len(tables)} tables:")
        for table in tables:
            print(f"  ‚Ä¢ LLM_EVAL.PUBLIC.{table}")
        return {'dry_run': True, 'tables_to_delete': len(tables)}
    
    print("üßπ CLEANING UP LLM ANALYSIS TABLES")
    print("‚ö†Ô∏è  WARNING: This will delete all LLM analysis data!")
    print("=" * 50)
    
    try:
        tables = list_all_output_tables()
        tables.append('LLM_EVALS_SUMMARY')  # Add master summary table
        
        deleted_count = 0
        failed_count = 0
        
        for table in tables:
            try:
                session.sql(f"DROP TABLE IF EXISTS LLM_EVAL.PUBLIC.{table}").collect()
                print(f"  ‚úÖ Deleted: {table}")
                deleted_count += 1
            except Exception as e:
                print(f"  ‚ùå Failed to delete {table}: {str(e)}")
                failed_count += 1
        
        cleanup_summary = f"""
üßπ CLEANUP COMPLETE
{'=' * 20}
‚úÖ Deleted: {deleted_count} tables
‚ùå Failed: {failed_count} tables
Status: {'SUCCESS' if failed_count == 0 else 'PARTIAL'}
"""
        
        print(cleanup_summary)
        
        return {
            'success': failed_count == 0,
            'deleted_count': deleted_count,
            'failed_count': failed_count,
            'cleanup_summary': cleanup_summary
        }
        
    except Exception as e:
        error_msg = f"Cleanup failed: {str(e)}"
        print(f"‚ùå {error_msg}")
        return {
            'success': False,
            'error': error_msg
        }


# Easy-to-use wrapper functions for main file integration
def run_llm_analysis(session: snowpark.Session, target_date=None):
    """Simple wrapper for running complete LLM analysis"""
    return llm_analysis_full_pipeline(session, target_date)


def test_llm_analysis(session: snowpark.Session, department='CC_Resolvers', target_date=None, prompts=['*'], metrics=['*']):
    """Simple wrapper for testing LLM analysis with dynamic prompt/metric selection"""
    return llm_analysis_quick_test(session, department, target_date, prompts, metrics)


def check_llm_status(session: snowpark.Session):
    """Simple wrapper for checking LLM status"""
    return llm_analysis_status_check(session)


def validate_llm_setup(session: snowpark.Session):
    """Simple wrapper for validating LLM setup"""
    return main_llm_validate(session)


# For adding to main snowflake file - examples of how to use
INTEGRATION_EXAMPLES = """
# Add these lines to your main snowflake file to use LLM analysis:

# Import the integration module
from snowflake_llm_integration import (
    run_llm_analysis,
    test_llm_analysis, 
    check_llm_status,
    validate_llm_setup
)

# Example usage in main function:
def main(session):
    # Option 1: Run complete analysis
    result = run_llm_analysis(session, '2025-07-22')
    return result['pipeline_summary']
    
    # Option 2: Test with single department
    result = test_llm_analysis(session, 'CC_Resolvers', '2025-07-22')
    return result['test_summary']
    
    # Option 3: Check system status
    result = check_llm_status(session)
    return result['status_summary']
    
    # Option 4: Validate setup
    result = validate_llm_setup(session)
    return f"LLM Setup: {result['overall_status']}"
"""