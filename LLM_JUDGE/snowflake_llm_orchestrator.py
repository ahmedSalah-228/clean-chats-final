"""
LLM Analysis Orchestrator Module for Snowflake Chat Analysis
Main module that orchestrates the entire LLM analysis workflow
Can be imported into the main snowflake file for easy execution
"""

import snowflake.snowpark as snowpark
from datetime import datetime, timedelta
import traceback
from snowflake_llm_config import (
    get_snowflake_llm_departments_config, 
    list_all_departments, 
    get_department_prompt_types,
    list_all_output_tables
)
from snowflake_llm_processor import (
    process_department_llm_analysis,
    update_llm_master_summary,
    test_llm_single_prompt,
    format_error_details
)


def analyze_llm_conversations_all_departments(session: snowpark.Session, target_date=None, department_filter=None):
    """
    Analyze LLM conversations for all departments - main orchestrator function
    
    Args:
        session: Snowflake session
        target_date: Target date for analysis (defaults to yesterday)
        department_filter: Optional specific department to process (for testing)
    
    Returns:
        Analysis results dictionary
    """
    print("\nü§ñ LLM ANALYSIS: PROCESSING ALL DEPARTMENTS")
    print("=" * 60)
    
    # Set target date if not provided
    if target_date is None:
        target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    print(f"üìÖ Target date: {target_date}")
    
    if department_filter:
        print(f"üéØ Department filter: {department_filter}")
    
    print("=" * 60)
    
    departments_config = get_snowflake_llm_departments_config()
    department_results = {}
    
    # Get list of departments to process
    departments_to_process = [department_filter] if department_filter else list(departments_config.keys())
    
    total_departments = len(departments_to_process)
    processed_departments = 0
    successful_departments = 0
    
    for department_name in departments_to_process:
        if department_name not in departments_config:
            print(f"‚ö†Ô∏è  Department {department_name} not found in configuration")
            continue
            
        try:
            # Process department (includes all its prompts)
            dept_results, success = process_department_llm_analysis(
                session, department_name, target_date
            )
            
            department_results[department_name] = dept_results
            processed_departments += 1
            
            if success:
                successful_departments += 1
            
        except Exception as e:
            error_msg = f"LLM analysis failed: {str(e)}"
            error_details = format_error_details(e, f"DEPARTMENT PROCESSING - {department_name}")
            print(f"  ‚ùå {department_name}: {error_msg}")
            print(error_details)
            department_results[department_name] = {'error': error_msg, 'traceback': str(e)}
            processed_departments += 1
    
    # Create summary and update master table
    print(f"\nüìä CREATING MASTER SUMMARY...")
    master_success = update_llm_master_summary(session, department_results, target_date)
    
    # Generate final summary
    total_conversations = 0
    total_processed = 0
    total_prompts = 0
    successful_prompts = 0
    
    for dept_name, dept_results in department_results.items():
        if 'error' not in dept_results:
            for prompt_type, prompt_results in dept_results.items():
                if isinstance(prompt_results, dict) and 'total_conversations' in prompt_results:
                    total_conversations += prompt_results.get('total_conversations', 0)
                    total_processed += prompt_results.get('processed_count', 0)
                    total_prompts += 1
                    if prompt_results.get('processed_count', 0) > 0:
                        successful_prompts += 1
    
    overall_success_rate = (total_processed / total_conversations * 100) if total_conversations > 0 else 0
    
    summary = f"""
üéØ LLM ANALYSIS - SUMMARY
{'=' * 50}
üìÖ Date: {target_date}
üè¢ Departments processed: {processed_departments}/{total_departments}
‚úÖ Successful departments: {successful_departments}/{processed_departments}

üìä OVERALL METRICS:
   üí¨ Total conversations: {total_conversations:,}
   ‚úÖ Successfully processed: {total_processed:,} ({overall_success_rate:.1f}%)
   üéØ Total prompts: {total_prompts}
   ‚úÖ Successful prompts: {successful_prompts}/{total_prompts}

üíæ OUTPUT:
   üìã Master summary: LLM_EVALS_SUMMARY {'‚úÖ' if master_success else '‚ùå'}
   üìä Raw data tables: {len(list_all_output_tables())} tables

üåü LLM Analysis Complete!
   Ready for review and further analysis
"""
    
    print(summary)
    
    return {
        'summary': summary,
        'department_results': department_results,
        'master_success': master_success,
        'statistics': {
            'target_date': target_date,
            'processed_departments': processed_departments,
            'successful_departments': successful_departments,
            'total_conversations': total_conversations,
            'total_processed': total_processed,
            'total_prompts': total_prompts,
            'successful_prompts': successful_prompts,
            'overall_success_rate': overall_success_rate
        }
    }


def analyze_llm_single_department(session: snowpark.Session, department_name, target_date=None, prompts=['*'], metrics=['*']):
    """
    Analyze LLM conversations for a single department
    
    Args:
        session: Snowflake session
        department_name: Department name to process
        target_date: Target date for analysis
    
    Returns:
        Single department analysis results
    """
    print(f"\nü§ñ LLM ANALYSIS: PROCESSING SINGLE DEPARTMENT - {department_name}")
    print("=" * 60)
    
    # Set target date if not provided
    if target_date is None:
        target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    print(f"üìÖ Target date: {target_date}")
    print("=" * 60)
    
    try:
        # Process the department
        dept_results, success = process_department_llm_analysis(session, department_name, target_date, selected_prompts=prompts)
        
        if success:
            # Update master summary for this department only
            department_results = {department_name: dept_results}
            master_success = update_llm_master_summary(session, department_results, target_date, selected_metrics=metrics)
            
            # Generate summary
            total_conversations = 0
            total_processed = 0
            successful_prompts = 0
            total_prompts = len(dept_results)
            
            for prompt_type, prompt_results in dept_results.items():
                if isinstance(prompt_results, dict) and 'total_conversations' in prompt_results:
                    total_conversations += prompt_results.get('total_conversations', 0)
                    total_processed += prompt_results.get('processed_count', 0)
                    if prompt_results.get('processed_count', 0) > 0:
                        successful_prompts += 1
            
            success_rate = (total_processed / total_conversations * 100) if total_conversations > 0 else 0
            
            summary = f"""
‚úÖ {department_name} - ANALYSIS COMPLETE
{'=' * 50}
üìÖ Date: {target_date}
üí¨ Conversations: {total_conversations:,}
‚úÖ Processed: {total_processed:,} ({success_rate:.1f}%)
üéØ Prompts: {successful_prompts}/{total_prompts} successful
üíæ Master summary: {'‚úÖ' if master_success else '‚ùå'}
"""
            print(summary)
            
            return {
                'success': True,
                'summary': summary,
                'department_results': dept_results,
                'statistics': {
                    'total_conversations': total_conversations,
                    'total_processed': total_processed,
                    'success_rate': success_rate,
                    'successful_prompts': successful_prompts,
                    'total_prompts': total_prompts
                }
            }
        else:
            error_msg = dept_results.get('error', 'Unknown error')
            print(f"‚ùå Failed to process {department_name}: {error_msg}")
            return {
                'success': False,
                'error': error_msg,
                'department_results': dept_results
            }
    
    except Exception as e:
        error_details = format_error_details(e, f"SINGLE DEPARTMENT ANALYSIS - {department_name}")
        print(error_details)
        return {
            'success': False,
            'error': str(e),
            'traceback': error_details
        }


def run_llm_test_suite(session: snowpark.Session, target_date=None):
    """
    Run a comprehensive test suite for LLM analysis
    
    Args:
        session: Snowflake session
        target_date: Target date for testing
    
    Returns:
        Test results
    """
    print("\nüß™ LLM ANALYSIS TEST SUITE")
    print("=" * 60)
    
    if target_date is None:
        target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    print(f"üìÖ Target date: {target_date}")
    print("=" * 60)
    
    test_results = {
        'configuration_tests': {},
        'department_tests': {},
        'prompt_tests': {},
        'overall_status': 'PENDING'
    }
    
    try:
        # Test 1: Configuration validation
        print("\nüîß Testing Configuration...")
        departments_config = get_snowflake_llm_departments_config()
        departments = list_all_departments()
        output_tables = list_all_output_tables()
        
        test_results['configuration_tests'] = {
            'departments_count': len(departments),
            'departments_with_prompts': len([d for d in departments if departments_config[d].get('llm_prompts')]),
            'total_output_tables': len(output_tables),
            'config_valid': True
        }
        
        print(f"    ‚úÖ {len(departments)} departments configured")
        print(f"    ‚úÖ {test_results['configuration_tests']['departments_with_prompts']} departments have LLM prompts")
        print(f"    ‚úÖ {len(output_tables)} output tables defined")
        
        # Test 2: Sample department processing
        print("\nüè¢ Testing Department Processing...")
        sample_departments = ['CC_Resolvers', 'Doctors']  # Test with common departments
        
        for dept in sample_departments:
            if dept in departments:
                print(f"  Testing {dept}...")
                try:
                    # Run analysis for single department
                    result = analyze_llm_single_department(session, dept, target_date)
                    test_results['department_tests'][dept] = {
                        'success': result['success'],
                        'processed_conversations': result.get('statistics', {}).get('total_processed', 0),
                        'error': result.get('error', None)
                    }
                    
                    if result['success']:
                        print(f"    ‚úÖ {dept}: {result['statistics']['total_processed']} conversations processed")
                    else:
                        print(f"    ‚ùå {dept}: {result.get('error', 'Unknown error')}")
                        
                except Exception as e:
                    test_results['department_tests'][dept] = {
                        'success': False,
                        'error': str(e)
                    }
                    print(f"    ‚ùå {dept}: Exception - {str(e)}")
        
        # Test 3: Specific prompt testing
        print("\nüéØ Testing Individual Prompts...")
        test_prompts = [
            ('CC_Resolvers', 'quality_assessment'),
            ('Doctors', 'medical_appropriateness')
        ]
        
        for dept, prompt_type in test_prompts:
            if dept in departments:
                prompt_key = f"{dept}_{prompt_type}"
                print(f"  Testing {dept}/{prompt_type}...")
                try:
                    test_llm_single_prompt(session, dept, prompt_type, target_date, sample_size=1)
                    test_results['prompt_tests'][prompt_key] = {'success': True}
                    print(f"    ‚úÖ {prompt_key}: Test completed")
                except Exception as e:
                    test_results['prompt_tests'][prompt_key] = {'success': False, 'error': str(e)}
                    print(f"    ‚ùå {prompt_key}: {str(e)}")
        
        # Determine overall status
        dept_success = all(result.get('success', False) for result in test_results['department_tests'].values())
        prompt_success = all(result.get('success', False) for result in test_results['prompt_tests'].values())
        
        if dept_success and prompt_success:
            test_results['overall_status'] = 'SUCCESS'
            print(f"\nüéâ TEST SUITE PASSED!")
        else:
            test_results['overall_status'] = 'PARTIAL'
            print(f"\n‚ö†Ô∏è  TEST SUITE PARTIALLY SUCCESSFUL")
        
    except Exception as e:
        test_results['overall_status'] = 'FAILED'
        error_details = format_error_details(e, "TEST SUITE")
        print(f"\n‚ùå TEST SUITE FAILED:")
        print(error_details)
        test_results['error'] = str(e)
    
    return test_results


def validate_llm_dependencies(session: snowpark.Session):
    """
    Validate that all LLM dependencies are available
    
    Args:
        session: Snowflake session
    
    Returns:
        Validation results
    """
    print("üîç VALIDATING LLM DEPENDENCIES")
    print("=" * 50)
    
    validation_results = {
        'snowflake_functions': {},
        'configuration': {},
        'permissions': {},
        'overall_status': 'PENDING'
    }
    
    try:
        # Test 1: Check if LLM functions exist
        print("üîß Testing Snowflake LLM functions...")
        
        llm_functions = ['openai_chat_system', 'gemini_chat_system']
        for func in llm_functions:
            try:
                test_query = f"SELECT {func}('Hello', 'You are a test assistant', 'gpt-4o-mini', 0.1, 10) AS test_response"
                result = session.sql(test_query).collect()
                validation_results['snowflake_functions'][func] = {
                    'available': True,
                    'response': 'Function callable'
                }
                print(f"    ‚úÖ {func}: Available and callable")
            except Exception as e:
                validation_results['snowflake_functions'][func] = {
                    'available': False,
                    'error': str(e)
                }
                print(f"    ‚ùå {func}: {str(e)}")
        
        # Test 2: Configuration validation
        print("\nüìã Testing configuration...")
        try:
            config = get_snowflake_llm_departments_config()
            departments = list_all_departments()
            validation_results['configuration'] = {
                'valid': True,
                'departments_count': len(departments),
                'config_loaded': True
            }
            print(f"    ‚úÖ Configuration valid: {len(departments)} departments")
        except Exception as e:
            validation_results['configuration'] = {
                'valid': False,
                'error': str(e)
            }
            print(f"    ‚ùå Configuration error: {str(e)}")
        
        # Test 3: Basic permissions
        print("\nüîí Testing permissions...")
        try:
            # Test table creation permissions
            test_table = "LLM_EVAL.PUBLIC.LLM_TEST_TABLE"
            session.sql(f"CREATE OR REPLACE TABLE {test_table} (test_col STRING)").collect()
            session.sql(f"DROP TABLE {test_table}").collect()
            
            validation_results['permissions'] = {
                'table_creation': True,
                'schema_access': True
            }
            print(f"    ‚úÖ Permissions: Table creation and schema access available")
        except Exception as e:
            validation_results['permissions'] = {
                'table_creation': False,
                'error': str(e)
            }
            print(f"    ‚ùå Permissions error: {str(e)}")
        
        # Determine overall status
        functions_ok = all(f.get('available', False) for f in validation_results['snowflake_functions'].values())
        config_ok = validation_results['configuration'].get('valid', False)
        permissions_ok = validation_results['permissions'].get('table_creation', False)
        
        if functions_ok and config_ok and permissions_ok:
            validation_results['overall_status'] = 'SUCCESS'
            print(f"\nüéâ ALL DEPENDENCIES VALIDATED!")
        else:
            validation_results['overall_status'] = 'FAILED'
            print(f"\n‚ùå DEPENDENCY VALIDATION FAILED")
            
    except Exception as e:
        validation_results['overall_status'] = 'ERROR'
        error_details = format_error_details(e, "DEPENDENCY VALIDATION")
        print(error_details)
        validation_results['error'] = str(e)
    
    return validation_results


# Main functions for easy import
def main_llm_analysis(session: snowpark.Session, target_date=None):
    """
    Main function for complete LLM analysis - can be called from main snowflake file
    """
    try:
        return analyze_llm_conversations_all_departments(session, target_date)
    except Exception as e:
        error_report = format_error_details(e, "MAIN LLM ANALYSIS")
        return {
            'summary': f"‚ùå LLM ANALYSIS FAILED: {str(e)}",
            'error': str(e),
            'traceback': error_report
        }


def main_llm_test(session: snowpark.Session, target_date=None):
    """
    Main function for LLM testing - can be called from main snowflake file
    """
    try:
        return run_llm_test_suite(session, target_date)
    except Exception as e:
        error_report = format_error_details(e, "MAIN LLM TEST")
        return {
            'overall_status': 'ERROR',
            'error': str(e),
            'traceback': error_report
        }


def main_llm_validate(session: snowpark.Session):
    """
    Main function for LLM validation - can be called from main snowflake file
    """
    try:
        return validate_llm_dependencies(session)
    except Exception as e:
        error_report = format_error_details(e, "MAIN LLM VALIDATE")
        return {
            'overall_status': 'ERROR',
            'error': str(e),
            'traceback': error_report
        }